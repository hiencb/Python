{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Implementing Back Propagation](https://github.com/nfmcclure/tensorflow_cookbook/tree/master/02_TensorFlow_Way/05_Implementing_Back_Propagation)\n",
    "\n",
    "One of the benefits of using TensorFlow, is that it can keep track of operations and automatically update model variables based on back propagation. In this recipe, we will introduce how to use this aspect to our advantage when training machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 800\n",
    "BATCH_SIZE = 80\n",
    "ITERATIONS = 10000\n",
    "LOG_INTERVAL = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOptimizer(learning_rate):\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "#     optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#     optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.15)\n",
    "#     optimizer = tf.train.ProximalAdagradOptimizer(learning_rate=learning_rate)\n",
    "#     optimizer = tf.train.ProximalGradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data, placeholders and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.random.normal(loc=6.5, scale=1.2, size=SAMPLE_SIZE)\n",
    "y_vals = np.vectorize(lambda x: x * 4.8 + np.random.normal(loc=0, scale=0.2))(x_vals)\n",
    "print(x_vals[: 6])\n",
    "print(y_vals[: 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE], name='x')\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE], name='y')\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.truncated_normal(shape=[1]), name='w')\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating predictions and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.multiply(w, x, name='predictions')\n",
    "loss = tf.nn.l2_loss(predictions - y, name='loss')\n",
    "print(predictions)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring optimizer and training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = getOptimizer(learning_rate=0.0005)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ITERATIONS):\n",
    "    rand_idx = np.random.choice(a=SAMPLE_SIZE, size=BATCH_SIZE, replace=False)\n",
    "    x_rand = x_vals[rand_idx]\n",
    "    y_rand = y_vals[rand_idx]\n",
    "    feedDict = {x: x_rand, y: y_rand}\n",
    "    sess.run(train, feed_dict=feedDict)\n",
    "    \n",
    "    if i == 0 or (i + 1) % LOG_INTERVAL == 0:\n",
    "        wRes, lossRes = sess.run([w, loss], feed_dict=feedDict)\n",
    "        print('#{} w = {}, loss = {}'.format(i + 1, wRes, lossRes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data, placeholders and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.78883276  1.0664738  -0.84926703 -2.65555492 -2.28857167 -1.30690461]\n",
      "[0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "half_size = SAMPLE_SIZE // 2\n",
    "x_vals = np.concatenate((np.random.normal(loc=-1, scale=1, size=half_size),\n",
    "                       np.random.normal(loc=3, scale=1, size=half_size)))\n",
    "\n",
    "y_vals = np.concatenate((np.repeat(a=0, repeats=half_size),\n",
    "                       np.repeat(a=1, repeats=half_size)))\n",
    "print(x_vals[: 6])\n",
    "print(y_vals[: 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(80, 1), dtype=float32)\n",
      "Tensor(\"y:0\", shape=(80, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE, 1], name='x')\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE, 1], name='y')\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"w/read:0\", shape=(1,), dtype=float32)\n",
      "Tensor(\"b/read:0\", shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.truncated_normal(mean=0, shape=[1]), name='w')\n",
    "b = tf.Variable(tf.truncated_normal(mean=2, shape=[1]), name='b')\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating outputs and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"outputs_1:0\", shape=(80, 1), dtype=float32)\n",
      "Tensor(\"xentropy_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output = tf.add(tf.multiply(w, x), b, name='outputs')\n",
    "xentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=output), name='xentropy')\n",
    "print(output)\n",
    "print(xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring optimizer and training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1)\n",
    "train = optimizer.minimize(xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 w = [ 0.76009929], b = [ 1.32883883], xentropy = 0.4946681559085846\n",
      "#501 w = [ 3.65443563], b = [-3.56057477], xentropy = 0.09474021196365356\n",
      "#1001 w = [ 3.89725685], b = [-3.71176076], xentropy = 0.024227600544691086\n",
      "#1501 w = [ 3.83745241], b = [-3.97193694], xentropy = 0.07012464851140976\n",
      "#2001 w = [ 3.84695029], b = [-3.9284234], xentropy = 0.11310561001300812\n",
      "#2501 w = [ 3.90799046], b = [-3.91635633], xentropy = 0.08712489902973175\n",
      "#3001 w = [ 3.83545256], b = [-3.99198675], xentropy = 0.02540026605129242\n",
      "#3501 w = [ 4.02067327], b = [-3.92227888], xentropy = 0.03159834071993828\n",
      "#4001 w = [ 3.97994828], b = [-3.98675418], xentropy = 0.022220898419618607\n",
      "#4501 w = [ 3.94023776], b = [-3.91804433], xentropy = 0.1165633350610733\n",
      "#5001 w = [ 3.94992208], b = [-4.01708364], xentropy = 0.009918822906911373\n",
      "#5501 w = [ 4.03207922], b = [-3.93867087], xentropy = 0.057833850383758545\n",
      "#6001 w = [ 3.92850971], b = [-3.98208046], xentropy = 0.08034975826740265\n",
      "#6501 w = [ 3.92189813], b = [-4.05074024], xentropy = 0.03631383925676346\n",
      "#7001 w = [ 3.88433862], b = [-4.08992815], xentropy = 0.13280461728572845\n",
      "#7501 w = [ 3.85645366], b = [-4.05110598], xentropy = 0.0926397293806076\n",
      "#8001 w = [ 3.86019969], b = [-4.08498335], xentropy = 0.02516186237335205\n",
      "#8501 w = [ 3.86156797], b = [-4.05877924], xentropy = 0.040535636246204376\n",
      "#9001 w = [ 3.87523675], b = [-3.96820188], xentropy = 0.03994059935212135\n",
      "#9501 w = [ 3.79046035], b = [-3.90453792], xentropy = 0.05192572996020317\n"
     ]
    }
   ],
   "source": [
    "for i in range(ITERATIONS):\n",
    "    rand_idx = np.random.choice(a=SAMPLE_SIZE, size=BATCH_SIZE, replace=False)\n",
    "    x_rand = np.expand_dims(a=x_vals[rand_idx], axis=1)\n",
    "    y_rand = np.expand_dims(a=y_vals[rand_idx], axis=1)\n",
    "    feedDict = {x: x_rand, y: y_rand}\n",
    "    sess.run(train, feed_dict=feedDict)\n",
    "    \n",
    "    if i % LOG_INTERVAL == 0:\n",
    "        wRes, bRes, xentropyRes = sess.run([w, b, xentropy], feed_dict=feedDict)\n",
    "        print('#{} w = {}, b = {}, xentropy = {}'.format(i + 1, wRes, bRes, xentropyRes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
