{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MNIST For ML Beginners](https://www.tensorflow.org/get_started/mnist/beginners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [The MNIST Data](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x105f409b0>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x106c5a630>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x106bbc7f0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:\n",
    "\n",
    "![](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "\n",
    "We can flatten this array into a vector of 28x28 = 784 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(55000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.92156869,  0.78039223,  0.        ,  0.91764712,  0.45882356,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.08627451,  0.80784321,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.99215692,  0.        ,  0.69803923,  0.99215692,\n",
       "         0.        ,  0.        ,  0.65882355,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(mnist.train.images))\n",
    "print(mnist.train.images.shape)\n",
    "mnist.train.images[: : 22000, : : 48]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image.\n",
    "\n",
    "For the purposes of this tutorial, we're going to want our labels as \"one-hot vectors\". A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. For example, 3 would be ``[0,0,0,1,0,0,0,0,0,0]``. Consequently, ``mnist.train.labels`` is a ``[55000, 10]`` array of floats.\n",
    "\n",
    "![](https://www.tensorflow.org/images/mnist-train-ys.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(55000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(mnist.train.labels))\n",
    "print(mnist.train.labels.shape)\n",
    "mnist.train.labels[: : 11000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{evidence}_i = \\sum_j W_{i,~ j} x_j + b_i$$\n",
    "\n",
    "$$y = \\text{softmax}(\\text{evidence})$$\n",
    "\n",
    "$$\\text{softmax}(x) = \\text{normalize}(\\exp(x))$$\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can picture our softmax regression as looking something like the following, although with a lot more xs. For each output, we compute a weighted sum of the xs, add a bias, and then apply softmax.\n",
    "\n",
    "![](https://www.tensorflow.org/images/softmax-regression-scalargraph.png)\n",
    "\n",
    "If we write that out as equations, we get:\n",
    "\n",
    "![](https://www.tensorflow.org/images/softmax-regression-scalarequation.png)\n",
    "\n",
    "We can \"vectorize\" this procedure, turning it into a matrix multiplication and vector addition. This is helpful for computational efficiency. (It's also a useful way to think.)\n",
    "\n",
    "![](https://www.tensorflow.org/images/softmax-regression-vectorequation.png)\n",
    "\n",
    "More compactly, we can just write\n",
    "\n",
    "$$y = \\text{softmax}(Wx + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Implement the Regression](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X:0\", shape=(?, 784), dtype=float32)\n",
      "Tensor(\"y:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"Variable/read:0\", shape=(784, 10), dtype=float32)\n",
      "Tensor(\"Variable_1/read:0\", shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=[None, 784], name='X') # None means that a dimension can be of any length\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='y')\n",
    "a = tf.Variable(tf.zeros(shape=[784, 10], dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros(shape=[10], dtype=tf.float32))\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very common, very nice function to determine the loss of a model is called \"cross-entropy.\" Cross-entropy arises from thinking about information compressing codes in information theory but it winds up being an important idea in lots of areas, from gambling to machine learning. It's defined as:\n",
    "\n",
    "$$H_{y'}(y) = -\\sum_i y'_i \\log(y_i)$$\n",
    "\n",
    "Where y is our predicted probability distribution, and yâ€² is the true distribution (the one-hot vector with the digit labels). In some rough sense, the cross-entropy is measuring how inefficient our predictions are for describing the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"y_pred:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"cross_entropy_1:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0x110e0d2b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numerically unstable way\n",
    "# y_prob = tf.nn.softmax(tf.matmul(X, a) + b, name='y_prob')\n",
    "# cross_entropy = tf.reduce_mean(- tf.reduce_sum(y * tf.log(y_prob), axis=1), name='cross_entropy')\n",
    "# print(y_prob)\n",
    "# print(cross_entropy)\n",
    "\n",
    "# more stable way\n",
    "y_pred = tf.add(tf.matmul(X, a), b, name='y_pred')\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_pred), name='cross_entropy')\n",
    "print(y_pred)\n",
    "print(cross_entropy)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "tf.summary.FileWriter(graph=sess.graph, logdir='logs/MNIST Beginner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    batch_Xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train, feed_dict={X: batch_Xs, y: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92430001"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare = tf.equal(tf.argmax(input=y_prob, axis=1), tf.argmax(input=y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(compare, tf.float32))\n",
    "dict_eval = {X: mnist.test.images, y: mnist.test.labels}\n",
    "# (yMax, y_probMax) = sess.run((tf.argmax(y, 1), tf.argmax(y_prob, 1)), feed_dict=dict_eval)\n",
    "# print(yMax[: 20])\n",
    "# print(y_probMax[: 20])\n",
    "# sess.run(y_prob, feed_dict=dict_eval)\n",
    "# sess.run(compare, feed_dict=dict_eval)\n",
    "sess.run(accuracy, feed_dict=dict_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
